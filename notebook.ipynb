{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8vU3grgHvpJW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #model will be trained on GPU 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2c0DVT5wyXi"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow==2.7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f5JZj_hEv9c0"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "%matplotlib inline\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input,Dense,Flatten,Dropout,Concatenate,Reshape,Conv2D,MaxPooling2D,UpSampling2D,Conv2DTranspose\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model,Sequential,load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.losses import categorical_crossentropy\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import fashion_mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "fybMBo1r3ekm",
        "outputId": "ae680151-d249-44a9-9668-98fc5f193cd0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ugv5p46Xv9pC"
      },
      "outputs": [],
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "assert x_train.shape == (60000, 28, 28)\n",
        "assert x_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wlFy1AEUv9rs"
      },
      "outputs": [],
      "source": [
        "label_dict = {\n",
        " 0: 'T-shirt/top',\n",
        " 1: 'Trouser',\n",
        " 2: 'Pullover',\n",
        " 3: 'Dress',\n",
        " 4: 'Coat',\n",
        " 5: 'Sandal',\n",
        " 6: 'Shirt',\n",
        " 7: 'Sneaker',\n",
        " 8: 'Bag',\n",
        " 9: 'Ankle boot',\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "jJ3KV0ozv9uO",
        "outputId": "d5f29e25-44c8-423b-c653-59bc51a40e07"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Class- T-shirt/top')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKAAAACuCAYAAACxxQZdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAR90lEQVR4nO2df7BV1XXHP18R/MVvnwgCASKgIRmDrfU3jo7EKvnhj87YmDTTGXWsM2ZsxdTaRlunkzROmhSnjdUxE4tJSGw6jUlssI2msfgjTQXGRiMVFUFBfsgP4YEiIKt/nPOce/bZ777z7r3v7Xsf6zPz5t29zzrnrPtY7LP2OmvvJTPDcVJxWGoFnEMbN0AnKW6ATlLcAJ2kuAE6SXEDdJIyJAxQ0h2Svptaj2aQtFjSl+oc3y3pg4Op02DQMQYo6TOSluf/EBslPSLp3DbQ65Fcp92S9kvaV9O+t1X3MbORZramjh5RA5Z0gqT1+ee1kua3SqdWcHhqBaogaSFwK3A98B/APuBi4FLgyYSqYWaX9HyWtBhYb2a3DaYOkobVObwA+PfB0qW/tP0IKGkM8NfADWb2QzPbY2b7zexhM/vTXs75F0mbJO2UtEzSh2uOLZD0gqRuSRskfSHv75L0b5LekrRd0hOSWvr3UcYiSVsk7ZL0nKSP1IiMk/TTXLdfSTqx5lyTNDP/vFjSPZKWStoDXAN8FrglH3kfrrnmAmCppO8AHwAezmVuya/1KUm/yb/345I+VHPPtZL+PP977ZD0T5KObOXfBDNr6x+yke4AcHgdmTuA79a0rwZGAUcAdwHP1hzbCMzLP48Dfiv//BXgXmB4/jMPUD91XQx8qc7x3wVWAGMBAR8CJtWcuw04nezJtAR4sOZcA2bWyO4EziEbRI6M3Tv/HluBUXl7LTC/5vhsYA/wsVz2FuBlYESN/PPAVGA88FS979fIT9uPgMCxwFYzO1D1BDO738y6zexdMuP8aD6SAuwH5kgabWY7zGxlTf8kYFo+wj5hrX9Rvp/sP8bJZMa9ysw21hx/yMz+J/+uS4C5da71YzN7yswOmtneXmTOA/7XzLp7Of77wE/N7FEz2w98DTgKOLtG5htm9rqZbQe+DFzV57fsB51ggNuALkmV/FVJwyTdKekVSbvI/hcDdOW/f4/ssbRO0n9JOivv/1uy//0/k7RG0q359T5bM6l4pD+K54+2nnPnmdl/At8A7ga2SLpP0uiaUzbVfH4bGFnn8q9XUGEBsLTO8ROAdT0NMzuYX3dyL/dZl5/TMjrBAH8JvAtcVlH+M2STk/nAGGB63i8AM3vGzC4FJgA/An6Q93eb2c1m9kHgU8BCSRea2RLLZqAjrWbCUQUz+3DNuU/kfX9vZr8NzCF7BEb92CqX76MNZQMMZd4ApvU0JInscbuhRmZqzecP5Oe0jLY3QDPbCfwlcLekyyQdLWm4pEskfTVyyigyg90GHA38Tc8BSSPyEW1M/sjZBRzMj31C0sz8H2En8F7PsVYh6XcknSFpOJnvtbeF99gMvB8nlDQDOMLMVvUmQ/af7+OSLsx1upnsb/d0jcwNkqZIGg98EfjnFukLdIABApjZ14GFwG3Am2SPhc+TjWAh3yZ7VGwAXgD+Ozj+OWBt/ni+nmz2CDALeAzYTTbq/qOZ/aK134TRwDeBHbmO28ge/a3gW2S+7VuSfgR8nPLj9yvAbbnMF8zsReAPgH8gm6x8Evikme2rOed7wM+ANcArQK/B8kZQ6/1spx2QtJRsAlHPB+zrGmuBa83ssZYpFtARI6DTEI8DrR7BW05HvAlx+o+ZxfzjtsMfwU5SmnoES7pY0ouSXu6JmzlOf2h4BMxfgK8me42zHngGuMrMXmides5Qpxkf8HTgZctThCQ9SBYA7tUAJSV93h92WHHAP+GEclB/5Mjiy4dt27aVZN58883WKtZPxo0bV2h3dXWVZHbu3Flob9myZUB1qsBWMzsu7GzGACdTfE2zHjijiesNOEcddVShvXDhwpLM2WefXWg/8MADJZl77rmntYr1k/nziyl91157bUnmkUeKbw3vuuuuAdWpAutinQM+C5Z0HXDdQN/H6UyaMcANFN8TTqH4DhEAM7sPuA/SP4Kd9qOZWfAzwCxJMySNAD4N/KQ1ajmHCk3FASUtIEv4HAbcb2Zf7kN+0EbAe+8tL8c477zzCu1hw8qZ7Js3by6058yZU5LZunVrof366+XMqNWrVxfau3btKsmMHz++1Bf6oCNGjCjJjB49utB+441ygko4mYrpeN11Rc9ozZpel5y0ghVmdlrY2ZQPmL9nbPhdo+P4u2AnKW6ATlIG9V3wQPqAF1xwQaF9663lN4NhUHnUqFElmTBYHcYOAY47rhhPPfroo0symzZtKrRXrFhRkjnttJJLxJFHFhedhQFlKPupEyZMKMls37690B47dmxJpru7uFTk8ssvL8m0kKgP6COgkxQ3QCcpboBOUtwAnaQMmYzoiy66qNBeu3ZtSeaII44otA8cKK91P/zw4p8kDDrHzssW0hUJg9yxgPbeveX15Hv27Cm0w4kCwOTJkwvtt99+uyQTTqY2bCi9JS0FtM8555ySzFNPPVXqayU+AjpJcQN0kuIG6CRlyPiAYXZz7OV/6APu37+/JBP6buE5AO+++26hHfptAMOHDy+0Y37ie++9V+oL/bJYkDv0+WJ+YviCIfQJYzLz5s0rybgP6Axp3ACdpDT1CM63bugm28jnQOxdn+PUoxU+4AVmVg6WOU4FOnISEnOoQ+c9lkUS9oWZJzHCwHRvfSHhJGTfvn19ykD5u8XuFcrErvPOO+/0qePBg8Wd4WbPnt3nOa2mWR/QyHYUXZGvfnOcftHsCHiumW2QNAF4VNL/mdmyWgFflunUo6kR0Mw25L+3AA+R7ZYQytxnZqf5BMWJ0fAIKOkY4DAz684/X0RWz2PAmTFjRqmvSiZz6APu2LGjJBP6XMcee2xJJkxGiAWrw8BzzN+MBafD4HjMBwzPC325WF8sYSEkTHIYDJp5BB8PPJT/MQ4HvmdmbVuRx2lPGjbAfFOij7ZQF+cQxN+EOElxA3SS0pGB6IkTJ5b6wgyVmGMeOu/r1pV3DAuzYXbv3t3ndY455piSTDhRiekTy8YJJx2xyUN4rfC7Q3lZaCyrJlyWGtsLMVyC2uq9EX0EdJLiBugkxQ3QSUpH+oCxPZE3btxYaI8ZM6YkE2b8LlmypCQTbnU2adKkkkwYeI69+A/9u9gWKLGM6DBpIZZoEF47tv/zmWeeWWjHfNBVq1YV2mFCB8BJJ51UaLsP6Awp3ACdpLgBOklxA3SS0pGTkDA4CuU9kcP9AqE8eYntz7dsWSGdkVNOOaUk89ZbbxXaMQc/zM6JBZ1j+z+HgfBYFk24t/Rrr71WkgkD2GecUS7hEl47to/03LlzC+0nn3yyJNMMPgI6SXEDdJLSpwFKul/SFknP1/SNl/SopJfy3+PqXcNxeqPPPaIlnQfsBr5tZh/J+74KbDezO/MyrePM7M/6vNkA7hE9bdq0QnvRokUlmRtvvLHQvvrqq0syYVZwbB/pcNuPmC8XElvJF8uIDpMYYokOxx9/fKEdC2hfeeWVhfZNN91UkpkyZUqhff3115dkYokODdLYHtH5IqPtQfelQE8VvweAy5pWzzkkadQHPN7Met59bSJLz3ecftN0GMbMrN6j1ZdlOvVodATcLGkSQP6712rIvizTqUejI+BPgD8E7sx//7hlGjVImN18xRVX9HnOc889V+oLM2bWr19fkgknD7GJXChTdRISTmhi+xyGS0VjRRfDQjW33357SaYdqBKG+T7wS+AkSeslXUNmeB+T9BIwP287Tr/pcwQ0s6t6OXRhi3VxDkH8TYiTlI5MRoj5TqGPFfO5woSAmA8YroKL+XdVtkersioupmPoz8XuHyYahAHlqsR8x5BYkLuV+AjoJMUN0EmKG6CTFDdAJykdOQmpssQx5vSHxArMhMT2dg4ziWNFD6tMJmKTqVDvWEZ0qHcs27oK4b36yowaCHwEdJLiBugkxQ3QSUpH+oBViAVZQ1+tSgA55l+FWcoxmSrFsWOB6NCXje11HWYpr169uiRThSpJFQONj4BOUtwAnaQ0uiruDkkbJD2b/ywYWDWdoUqVEXAxcHGkf5GZzc1/lrZWLedQoUo+4DJJ0wdelcEnrLIO5QlFlYKGsaWTVYLDsWB5ODGKXSecvMQyVsIMmSqZ3Sloxgf8vKRf549oX5juNESjBngPcCIwF9gIfL03QUnXSVouaXmD93KGMA0ZoJltNrP3zOwg8E0iRQprZH1VnNMrDRlgz5LMnMuB53uTdZx69DkJyVfFnQ90SVoP/BVwvqS5ZAWr1wJ/NIA6NkSVqP5ZZ51V6gud/ip7+MX2TwnfYMRkqkxCYoVqwvvH3pZMmDCh0I5NQqpMZgaaRlfFfWsAdHEOQfxNiJMUN0AnKUM2G6ZKRvTMmTNLfWHWSqzIX+inxfy7sOhgLKBcRcdYIDz0C2N+alhgZuXKlSWZFNkvIT4COklxA3SS4gboJMUN0EnKkJmEhEHVmIMfTgzCYC3A3r17C+2qyylDwpT82PLOWOC3SoGbKjLhJCRGlUnQQOMjoJMUN0AnKW6ATlKGjA9YxS8LK4Jv27atJBMWQuzu7i7JhMVrqvhpMWJLR8PvEZMJ/dLQtwU48cQT+7x/6APG/oYDHaz2EdBJihugk5QqyzKnSvqFpBck/UbSH+f9XrDQaZoqI+AB4GYzmwOcCdwgaQ5wK/BzM5sF/DxvO06/qJKQupFs4RFm1i1pFTCZrGDh+bnYA8DjQJ8VMweKKpOQqVOnFtqxSpih0x0GlKGcfRJz1EOZ2HXCoHfsWrFs53BiFNt3JpwYxfbBCWWq7FXTavrlA+brg08FfoUXLHRaQOUwjKSRwL8Cf2Jmu2pHnHoFC71YoVOPSiOgpOFkxrfEzH6Yd1cqWOjLMp16VFkVJ7JFSKvM7O9qDrVdwcK+OPnkkwvtMDANsGPHjkJ73Ljy5D5MLIgFgsO+mC8X8wHDa48dO7ZPmdh1wkzqMWPGlGS2bt1aaKfYqqPKI/gc4HPAc5Kezfv+gszwfpAXL1wHXNnL+Y7TK1VmwU8Cvf3X8IKFTlP4mxAnKW6ATlKGTDZMFcaPH19ox5Y8hsHZmPMeZtHEJiFhQDkW5I0Fh8NqnbH7h4HoWMZM2Ddx4sSSTDgJSYGPgE5S3ACdpLgBOkkZMj5glSDqjBkzCu3YSrXwOrH9n9esWVNoxxINQqoEvWM6xRImqmz9Fn6PkSNH9qljikC0j4BOUtwAnaS4ATpJcQN0kjJkJiFVCLN7q+ztHJuoVNlHOpy8hEFwgFdffbXUF7tWSJW9nWNB7v5edzDwEdBJihugk5RmlmV6xUynaar4gD3LMldKGgWskPRofmyRmX1t4NRrLaE/V8V32rKlvNIg3NIi5kuG14nda/v27aW+cE/qMDkBqm1FFxLLmg5JsV1bM8syHadpmlmWCRUqZnqxQqcelQ0wXJZJxYqZvirOqUfDyzL7UzHTcXqj4WWZkibV7IzQERUzZ8+eXWjHljyGQeaYTLhUMxY87urqKrRj2TCzZs0q9YX7Vp966qklmaeffrrQjmXMhJktsYB6O9DMssyr2r1iptP+NLMsc2nr1XEONfxNiJOUIZOMUCWIunx5MRIU+mlQDjzHArjharLY9miTJxdDpZMmTSrJxAoIhv7k9OnTSzLhirtYUeu5c+cW2ps2bSrJhKQIRPsI6CTFDdBJihugkxQ3QCcpGsyq2ZLeJNvKrQtIvy9E/+lEvdtF52lmdlzYOagG+P5NpeWd+G64E/Vud539EewkxQ3QSUoqA7wv0X2bpRP1bmudk/iAjtODP4KdpAy6AUq6WNKLkl6W1Jb15fIlBlskPV/T19bFGTu1qOSgGqCkYcDdwCXAHLKcwjmDqUNFFgMXB33tXpyxI4tKDvYIeDrwspmtMbN9wINkRQ/bCjNbBoRrJi8lK8pI/vuyQVWqD8xso5mtzD93A7VFJdtW78E2wMnA6zXt9XTOEs+OKc7YSUUlfRLSAJaFDtoyfBBZvfg+7aj3YBvgBqC2aO+UvK8TqFScMSXNFJVMxWAb4DPALEkzJI0APk1W9LAT6CnOCG1YnLFCUUloQ70xs0H9ARYAq4FXgC8O9v0r6vh9ssX2+8n81GuAY8lmkS8BjwHjU+sZ6Hwu2eP118Cz+c+Cdtfb34Q4SfFJiJMUN0AnKW6ATlLcAJ2kuAE6SXEDdJLiBugkxQ3QScr/A0J3T1DwPHjHAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(figsize=[5,5])\n",
        "\n",
        "# Seeing what the images look like\n",
        "plt.subplot(121)\n",
        "curr_img = np.reshape(x_train[10], (28,28))\n",
        "curr_lbl = y_train[10]\n",
        "plt.imshow(curr_img, cmap='gray')\n",
        "plt.title(f\"Class- {label_dict[curr_lbl]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wUZdgUGuv9y7",
        "outputId": "f10b0bf6-fcb2-47db-d3c1-249c043d2661"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((60000, 28, 28, 1), (10000, 28, 28, 1))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# Data Preprocessing\n",
        "x_train = x_train.reshape(-1, 28,28, 1)\n",
        "x_test = x_test.reshape(-1, 28,28, 1)\n",
        "x_train.shape, x_test.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ax9yU08gzZER",
        "outputId": "fb76a787-c648-44f5-f045-a4a88d3bc040"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(dtype('uint8'), dtype('uint8'), 255, 255)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Validation dtypes, range\n",
        "x_train.dtype, x_test.dtype, np.max(x_train), np.max(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qf15_oXk4g-3",
        "outputId": "d2d9a823-4b99-440a-a6fd-e2481d8b32af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1.0, 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "# Scaling\n",
        "x_train = x_train / np.max(x_train)\n",
        "x_test = x_test / np.max(x_test)\n",
        "np.max(x_train), np.max(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Autoencoder to understand the data"
      ],
      "metadata": {
        "id": "K9ogxg64DE49"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "IcED_SpezunB"
      },
      "outputs": [],
      "source": [
        "## Spliting training data to train and validate\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_X,valid_X,train_ground,valid_ground = train_test_split(x_train,\n",
        "                                                             x_train,\n",
        "                                                             test_size=0.2,\n",
        "                                                             random_state=13)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-ePz_azgz83S"
      },
      "outputs": [],
      "source": [
        "# Basic autoenc params\n",
        "batch_size = 64\n",
        "epochs = 100\n",
        "inChannel = 1\n",
        "x, y = 28, 28\n",
        "input_img = Input(shape = (x, y, inChannel))\n",
        "num_classes = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "L30v3zz8z9Sh"
      },
      "outputs": [],
      "source": [
        "# Enc/Dec functions, we will only use the enc part for later\n",
        "def encoder(input_img):\n",
        "    #encoder\n",
        "    #input = 28 x 28 x 1 (wide and thin)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img) #28 x 28 x 32\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv1)\n",
        "    conv1 = BatchNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #14 x 14 x 32\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1) #14 x 14 x 64\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
        "    conv2 = BatchNormalization()(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #7 x 7 x 64\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2) #7 x 7 x 128 (small and thick)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
        "    conv3 = BatchNormalization()(conv3)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv3) #7 x 7 x 256 (small and thick)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
        "    conv4 = BatchNormalization()(conv4)\n",
        "    return conv4\n",
        "\n",
        "def decoder(conv4):    \n",
        "    #decoder\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv4) #7 x 7 x 128\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv5)\n",
        "    conv5 = BatchNormalization()(conv5)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv5) #7 x 7 x 64\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv6)\n",
        "    conv6 = BatchNormalization()(conv6)\n",
        "    up1 = UpSampling2D((2,2))(conv6) #14 x 14 x 64\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(up1) # 14 x 14 x 32\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    conv7 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv7)\n",
        "    conv7 = BatchNormalization()(conv7)\n",
        "    up2 = UpSampling2D((2,2))(conv7) # 28 x 28 x 32\n",
        "    decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(up2) # 28 x 28 x 1\n",
        "    return decoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMoYj4Bn0BUC",
        "outputId": "4df821ce-4ec3-4a37-b53c-27a9947a15bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
            "                                                                 \n",
            " conv2d_71 (Conv2D)          (None, 28, 28, 32)        320       \n",
            "                                                                 \n",
            " batch_normalization_70 (Bat  (None, 28, 28, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_72 (Conv2D)          (None, 28, 28, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_71 (Bat  (None, 28, 28, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 14, 14, 32)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_73 (Conv2D)          (None, 14, 14, 64)        18496     \n",
            "                                                                 \n",
            " batch_normalization_72 (Bat  (None, 14, 14, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_74 (Conv2D)          (None, 14, 14, 64)        36928     \n",
            "                                                                 \n",
            " batch_normalization_73 (Bat  (None, 14, 14, 64)       256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  (None, 7, 7, 64)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_75 (Conv2D)          (None, 7, 7, 128)         73856     \n",
            "                                                                 \n",
            " batch_normalization_74 (Bat  (None, 7, 7, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_76 (Conv2D)          (None, 7, 7, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_75 (Bat  (None, 7, 7, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_77 (Conv2D)          (None, 7, 7, 256)         295168    \n",
            "                                                                 \n",
            " batch_normalization_76 (Bat  (None, 7, 7, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_78 (Conv2D)          (None, 7, 7, 256)         590080    \n",
            "                                                                 \n",
            " batch_normalization_77 (Bat  (None, 7, 7, 256)        1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_79 (Conv2D)          (None, 7, 7, 128)         295040    \n",
            "                                                                 \n",
            " batch_normalization_78 (Bat  (None, 7, 7, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_80 (Conv2D)          (None, 7, 7, 128)         147584    \n",
            "                                                                 \n",
            " batch_normalization_79 (Bat  (None, 7, 7, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_81 (Conv2D)          (None, 7, 7, 64)          73792     \n",
            "                                                                 \n",
            " batch_normalization_80 (Bat  (None, 7, 7, 64)         256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_82 (Conv2D)          (None, 7, 7, 64)          36928     \n",
            "                                                                 \n",
            " batch_normalization_81 (Bat  (None, 7, 7, 64)         256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " up_sampling2d_2 (UpSampling  (None, 14, 14, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_83 (Conv2D)          (None, 14, 14, 32)        18464     \n",
            "                                                                 \n",
            " batch_normalization_82 (Bat  (None, 14, 14, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_84 (Conv2D)          (None, 14, 14, 32)        9248      \n",
            "                                                                 \n",
            " batch_normalization_83 (Bat  (None, 14, 14, 32)       128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " up_sampling2d_3 (UpSampling  (None, 28, 28, 32)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_85 (Conv2D)          (None, 28, 28, 1)         289       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,758,657\n",
            "Trainable params: 1,755,841\n",
            "Non-trainable params: 2,816\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Train the autoenc\n",
        "autoencoder = Model(input_img, decoder(encoder(input_img)))\n",
        "autoencoder.compile(loss='mean_squared_error', optimizer = RMSprop())\n",
        "autoencoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uutvTNpE0DO2",
        "outputId": "c99b9d2e-1c32-4fde-e38b-3e90d1a8e0f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "750/750 [==============================] - 26s 29ms/step - loss: 0.0193 - val_loss: 0.0159\n",
            "Epoch 2/100\n",
            "750/750 [==============================] - 17s 22ms/step - loss: 0.0086 - val_loss: 0.0090\n",
            "Epoch 3/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0067 - val_loss: 0.0059\n",
            "Epoch 4/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0059 - val_loss: 0.0056\n",
            "Epoch 5/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0050 - val_loss: 0.0062\n",
            "Epoch 6/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0046 - val_loss: 0.0039\n",
            "Epoch 7/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0042 - val_loss: 0.0071\n",
            "Epoch 8/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0039 - val_loss: 0.0055\n",
            "Epoch 9/100\n",
            "750/750 [==============================] - 14s 18ms/step - loss: 0.0037 - val_loss: 0.0037\n",
            "Epoch 10/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0034 - val_loss: 0.0099\n",
            "Epoch 11/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0032 - val_loss: 0.0032\n",
            "Epoch 12/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0031 - val_loss: 0.0037\n",
            "Epoch 13/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0030 - val_loss: 0.0042\n",
            "Epoch 14/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0028 - val_loss: 0.0032\n",
            "Epoch 15/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0027 - val_loss: 0.0034\n",
            "Epoch 16/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0026 - val_loss: 0.0029\n",
            "Epoch 17/100\n",
            "750/750 [==============================] - 14s 18ms/step - loss: 0.0025 - val_loss: 0.0028\n",
            "Epoch 18/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0024 - val_loss: 0.0023\n",
            "Epoch 19/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0023 - val_loss: 0.0023\n",
            "Epoch 20/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0022 - val_loss: 0.0028\n",
            "Epoch 21/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0022 - val_loss: 0.0022\n",
            "Epoch 22/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0021 - val_loss: 0.0019\n",
            "Epoch 23/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0020 - val_loss: 0.0025\n",
            "Epoch 24/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0020 - val_loss: 0.0028\n",
            "Epoch 25/100\n",
            "750/750 [==============================] - 14s 18ms/step - loss: 0.0019 - val_loss: 0.0021\n",
            "Epoch 26/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0019 - val_loss: 0.0018\n",
            "Epoch 27/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0018 - val_loss: 0.0561\n",
            "Epoch 28/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0018 - val_loss: 0.0017\n",
            "Epoch 29/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0018 - val_loss: 0.0019\n",
            "Epoch 30/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0017 - val_loss: 0.0018\n",
            "Epoch 31/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 32/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0017 - val_loss: 0.0017\n",
            "Epoch 33/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0016 - val_loss: 0.0015\n",
            "Epoch 34/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 35/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0016 - val_loss: 0.0016\n",
            "Epoch 36/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0016 - val_loss: 0.0018\n",
            "Epoch 37/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 38/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0015 - val_loss: 0.0016\n",
            "Epoch 39/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0015 - val_loss: 0.0015\n",
            "Epoch 40/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0015 - val_loss: 0.0017\n",
            "Epoch 41/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 42/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0014 - val_loss: 0.0016\n",
            "Epoch 43/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 44/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0014 - val_loss: 0.0015\n",
            "Epoch 45/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 46/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0014 - val_loss: 0.0014\n",
            "Epoch 47/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0013 - val_loss: 0.0015\n",
            "Epoch 48/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 49/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 50/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0013 - val_loss: 0.0014\n",
            "Epoch 51/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 52/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 53/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0013 - val_loss: 0.0017\n",
            "Epoch 54/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0013 - val_loss: 0.0013\n",
            "Epoch 55/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 56/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 57/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 58/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 59/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0012 - val_loss: 0.0013\n",
            "Epoch 60/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 61/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 62/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 63/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0012 - val_loss: 0.0014\n",
            "Epoch 64/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0011 - val_loss: 0.0014\n",
            "Epoch 65/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0011 - val_loss: 0.0015\n",
            "Epoch 66/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 67/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0011 - val_loss: 0.0017\n",
            "Epoch 68/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 69/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 70/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 71/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0011 - val_loss: 0.0013\n",
            "Epoch 72/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 73/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 74/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0011 - val_loss: 0.0012\n",
            "Epoch 75/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 76/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 77/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 78/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 79/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 0.0010 - val_loss: 0.0011\n",
            "Epoch 80/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 81/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 82/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0010 - val_loss: 0.0012\n",
            "Epoch 83/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 0.0010 - val_loss: 0.0013\n",
            "Epoch 84/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.9254e-04 - val_loss: 0.0011\n",
            "Epoch 85/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.8780e-04 - val_loss: 0.0011\n",
            "Epoch 86/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.8363e-04 - val_loss: 0.0013\n",
            "Epoch 87/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.7755e-04 - val_loss: 0.0011\n",
            "Epoch 88/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.7419e-04 - val_loss: 0.0011\n",
            "Epoch 89/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.6731e-04 - val_loss: 0.0011\n",
            "Epoch 90/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 9.5920e-04 - val_loss: 0.0012\n",
            "Epoch 91/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.5699e-04 - val_loss: 0.0010\n",
            "Epoch 92/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.5425e-04 - val_loss: 0.0010\n",
            "Epoch 93/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.4593e-04 - val_loss: 0.0012\n",
            "Epoch 94/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.4184e-04 - val_loss: 0.0010\n",
            "Epoch 95/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.3828e-04 - val_loss: 0.0011\n",
            "Epoch 96/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.3009e-04 - val_loss: 0.0010\n",
            "Epoch 97/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.3101e-04 - val_loss: 9.9434e-04\n",
            "Epoch 98/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.2459e-04 - val_loss: 0.0010\n",
            "Epoch 99/100\n",
            "750/750 [==============================] - 13s 18ms/step - loss: 9.2092e-04 - val_loss: 0.0010\n",
            "Epoch 100/100\n",
            "750/750 [==============================] - 13s 17ms/step - loss: 9.1869e-04 - val_loss: 0.0011\n"
          ]
        }
      ],
      "source": [
        "autoencoder_train = autoencoder.fit(train_X, train_ground, batch_size=batch_size,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the training part. \n",
        "Looks good, no overfitting since both losses are going down.\n",
        "\n"
      ],
      "metadata": {
        "id": "XK7yhqghDhOZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "zUSLRbI70N3Z",
        "outputId": "e695d5e8-0a6d-42d8-9692-767543495bfc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(*args, **kw)>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "loss = autoencoder_train.history['loss']\n",
        "val_loss = autoencoder_train.history['val_loss']\n",
        "epochs = range(100)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Y1fIk9w0efw"
      },
      "outputs": [],
      "source": [
        "autoencoder.save('./models/autoencoder.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training a classifer on top of the embeddings"
      ],
      "metadata": {
        "id": "OAWOjgqZD0Zd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Ha27Ef1PrzKl"
      },
      "outputs": [],
      "source": [
        "autoencoder = load_model('./models/autoencoder.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "r0LIjENfqpgn"
      },
      "outputs": [],
      "source": [
        "# OHE for labels (originally categorical)\n",
        "train_Y_one_hot = to_categorical(y_train)\n",
        "test_Y_one_hot = to_categorical(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "DL7YdODVn0lf"
      },
      "outputs": [],
      "source": [
        "## Since we have to use minimum labeled samples from training, we do it this way:\n",
        "## we iterate using n (labeled) samples at a time, see how our model performs and minimize n.\n",
        "## note: in the autoenc above we used all samples but we did not use any labels ;)\n",
        "\n",
        "def fc(enco):\n",
        "    flat = Flatten()(enco)\n",
        "    den = Dense(128, activation='relu')(flat)\n",
        "    out = Dense(num_classes, activation='softmax')(den)\n",
        "    return out\n",
        "\n",
        "def train_classifier(n):\n",
        "  \"\"\"\n",
        "  this fn defines the full model: encoder from autoencoder + simple classifier\n",
        "  creates a new 'full_model' classifier\n",
        "  the target is the class\n",
        "\n",
        "  Input: n=the number of samples to consider in training data \n",
        "  Returns: model\n",
        "  \"\"\"\n",
        "  assert n!=0\n",
        "\n",
        "  #we take a total of n samples: 0.2n in test -> 0.8n in train, shuffling is handled by train_test_split\n",
        "  train_X,valid_X,train_label,valid_label = train_test_split(x_train,train_Y_one_hot,train_size=int(0.8*n), test_size=int(0.2*n),random_state=13)\n",
        "  \n",
        "  print(np.shape(train_X), np.shape(valid_X)) #basic assert\n",
        "  \n",
        "  \n",
        "  encode = encoder(input_img)\n",
        "  full_model = Model(input_img,fc(encode))\n",
        "\n",
        "  #set weights\n",
        "  for l1,l2 in zip(full_model.layers[:19],autoencoder.layers[0:19]):\n",
        "    l1.set_weights(l2.get_weights())\n",
        "\n",
        "  #make the trained autoenc layers untrainable, (bias + speed reasons + more)\n",
        "  for layer in full_model.layers[0:19]:\n",
        "    layer.trainable = False\n",
        "\n",
        "  #compile the model\n",
        "  #the only var here is the size of training data\n",
        "  full_model.compile(loss=categorical_crossentropy, optimizer=Adam() ,metrics=['accuracy'])\n",
        "\n",
        "  classify_train = full_model.fit(train_X, train_label, batch_size=64,epochs=100,verbose=1,validation_data=(valid_X, valid_label))\n",
        "  full_model.save(f'./models/classification_complete_{n}.h5')\n",
        "  return full_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "2A7OoJYppYnV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "def calculate_metrics(model):\n",
        "  \"\"\"\n",
        "  takes in the full_model, returns loss,acc\n",
        "  \"\"\"\n",
        "  \n",
        "  # note: we could also use the eval method to calculate accuracy:\n",
        "  # test_eval = model.evaluate(x_test, test_Y_one_hot, verbose=0)\n",
        "  # model_loss, model_acc = test_eval[0], test_eval[1]\n",
        "  # we do rounding for more accurate results\n",
        "\n",
        "  y_pred = model.predict(x_test)\n",
        "  y_pred = np.argmax(np.round(y_pred),axis=1)\n",
        "\n",
        "  model_acc = accuracy_score(y_test, y_pred)\n",
        "  return model_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "5hc5pVVwpxVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ea6c07b-f7a0-4ee5-a7d3-00df2b457807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(80, 28, 28, 1) (20, 28, 28, 1)\n",
            "Epoch 1/100\n",
            "2/2 [==============================] - 1s 263ms/step - loss: 2.7754 - accuracy: 0.1250 - val_loss: 3.7006 - val_accuracy: 0.3000\n",
            "Epoch 2/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.6400 - accuracy: 0.8250 - val_loss: 3.3915 - val_accuracy: 0.4000\n",
            "Epoch 3/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0375 - accuracy: 0.9875 - val_loss: 4.9298 - val_accuracy: 0.4500\n",
            "Epoch 4/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.3985 - accuracy: 0.9500 - val_loss: 4.4841 - val_accuracy: 0.3500\n",
            "Epoch 5/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.1881 - accuracy: 0.9750 - val_loss: 3.4544 - val_accuracy: 0.3500\n",
            "Epoch 6/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0116 - accuracy: 0.9875 - val_loss: 3.1451 - val_accuracy: 0.5000\n",
            "Epoch 7/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 3.2690 - val_accuracy: 0.5500\n",
            "Epoch 8/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 3.3801 - val_accuracy: 0.6000\n",
            "Epoch 9/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 8.7007e-04 - accuracy: 1.0000 - val_loss: 3.5801 - val_accuracy: 0.5500\n",
            "Epoch 10/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 3.7391 - val_accuracy: 0.5000\n",
            "Epoch 11/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 3.7542 - val_accuracy: 0.5000\n",
            "Epoch 12/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 3.7124 - val_accuracy: 0.5000\n",
            "Epoch 13/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.9475e-04 - accuracy: 1.0000 - val_loss: 3.6685 - val_accuracy: 0.5000\n",
            "Epoch 14/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.9219e-04 - accuracy: 1.0000 - val_loss: 3.6286 - val_accuracy: 0.6000\n",
            "Epoch 15/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1.1754e-04 - accuracy: 1.0000 - val_loss: 3.5909 - val_accuracy: 0.6000\n",
            "Epoch 16/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 8.4028e-05 - accuracy: 1.0000 - val_loss: 3.5577 - val_accuracy: 0.6000\n",
            "Epoch 17/100\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 5.6758e-05 - accuracy: 1.0000 - val_loss: 3.5299 - val_accuracy: 0.6000\n",
            "Epoch 18/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 4.4302e-05 - accuracy: 1.0000 - val_loss: 3.5056 - val_accuracy: 0.6000\n",
            "Epoch 19/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.4760e-05 - accuracy: 1.0000 - val_loss: 3.4849 - val_accuracy: 0.6000\n",
            "Epoch 20/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 2.7204e-05 - accuracy: 1.0000 - val_loss: 3.4657 - val_accuracy: 0.6000\n",
            "Epoch 21/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 2.3573e-05 - accuracy: 1.0000 - val_loss: 3.4486 - val_accuracy: 0.6000\n",
            "Epoch 22/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 2.0080e-05 - accuracy: 1.0000 - val_loss: 3.4326 - val_accuracy: 0.6000\n",
            "Epoch 23/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1.7621e-05 - accuracy: 1.0000 - val_loss: 3.4190 - val_accuracy: 0.6500\n",
            "Epoch 24/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.5528e-05 - accuracy: 1.0000 - val_loss: 3.4072 - val_accuracy: 0.6500\n",
            "Epoch 25/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1.4269e-05 - accuracy: 1.0000 - val_loss: 3.3967 - val_accuracy: 0.6500\n",
            "Epoch 26/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1.2973e-05 - accuracy: 1.0000 - val_loss: 3.3876 - val_accuracy: 0.6500\n",
            "Epoch 27/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 1.2128e-05 - accuracy: 1.0000 - val_loss: 3.3796 - val_accuracy: 0.6500\n",
            "Epoch 28/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.1246e-05 - accuracy: 1.0000 - val_loss: 3.3726 - val_accuracy: 0.6500\n",
            "Epoch 29/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.0710e-05 - accuracy: 1.0000 - val_loss: 3.3663 - val_accuracy: 0.6500\n",
            "Epoch 30/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 1.0134e-05 - accuracy: 1.0000 - val_loss: 3.3608 - val_accuracy: 0.6500\n",
            "Epoch 31/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 9.6880e-06 - accuracy: 1.0000 - val_loss: 3.3559 - val_accuracy: 0.6500\n",
            "Epoch 32/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 9.2932e-06 - accuracy: 1.0000 - val_loss: 3.3515 - val_accuracy: 0.6500\n",
            "Epoch 33/100\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 8.9044e-06 - accuracy: 1.0000 - val_loss: 3.3474 - val_accuracy: 0.6500\n",
            "Epoch 34/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 8.5781e-06 - accuracy: 1.0000 - val_loss: 3.3433 - val_accuracy: 0.6500\n",
            "Epoch 35/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 8.3352e-06 - accuracy: 1.0000 - val_loss: 3.3394 - val_accuracy: 0.6500\n",
            "Epoch 36/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 8.0655e-06 - accuracy: 1.0000 - val_loss: 3.3358 - val_accuracy: 0.6500\n",
            "Epoch 37/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 7.8182e-06 - accuracy: 1.0000 - val_loss: 3.3326 - val_accuracy: 0.6500\n",
            "Epoch 38/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 7.6141e-06 - accuracy: 1.0000 - val_loss: 3.3296 - val_accuracy: 0.6500\n",
            "Epoch 39/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 7.4055e-06 - accuracy: 1.0000 - val_loss: 3.3270 - val_accuracy: 0.6500\n",
            "Epoch 40/100\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 7.2118e-06 - accuracy: 1.0000 - val_loss: 3.3245 - val_accuracy: 0.6500\n",
            "Epoch 41/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 7.0494e-06 - accuracy: 1.0000 - val_loss: 3.3218 - val_accuracy: 0.6500\n",
            "Epoch 42/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 6.9168e-06 - accuracy: 1.0000 - val_loss: 3.3193 - val_accuracy: 0.6500\n",
            "Epoch 43/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 6.7142e-06 - accuracy: 1.0000 - val_loss: 3.3168 - val_accuracy: 0.6500\n",
            "Epoch 44/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 6.5741e-06 - accuracy: 1.0000 - val_loss: 3.3140 - val_accuracy: 0.6500\n",
            "Epoch 45/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 6.4653e-06 - accuracy: 1.0000 - val_loss: 3.3114 - val_accuracy: 0.6500\n",
            "Epoch 46/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 6.2851e-06 - accuracy: 1.0000 - val_loss: 3.3089 - val_accuracy: 0.6500\n",
            "Epoch 47/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 6.1524e-06 - accuracy: 1.0000 - val_loss: 3.3064 - val_accuracy: 0.6500\n",
            "Epoch 48/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 6.0481e-06 - accuracy: 1.0000 - val_loss: 3.3038 - val_accuracy: 0.6500\n",
            "Epoch 49/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 5.9304e-06 - accuracy: 1.0000 - val_loss: 3.3015 - val_accuracy: 0.6500\n",
            "Epoch 50/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 5.7844e-06 - accuracy: 1.0000 - val_loss: 3.2992 - val_accuracy: 0.6500\n",
            "Epoch 51/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 5.7010e-06 - accuracy: 1.0000 - val_loss: 3.2968 - val_accuracy: 0.6500\n",
            "Epoch 52/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 5.5669e-06 - accuracy: 1.0000 - val_loss: 3.2945 - val_accuracy: 0.6500\n",
            "Epoch 53/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 5.4790e-06 - accuracy: 1.0000 - val_loss: 3.2922 - val_accuracy: 0.6500\n",
            "Epoch 54/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 5.3762e-06 - accuracy: 1.0000 - val_loss: 3.2899 - val_accuracy: 0.6500\n",
            "Epoch 55/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 5.2659e-06 - accuracy: 1.0000 - val_loss: 3.2876 - val_accuracy: 0.6500\n",
            "Epoch 56/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 5.1765e-06 - accuracy: 1.0000 - val_loss: 3.2853 - val_accuracy: 0.6500\n",
            "Epoch 57/100\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 5.0990e-06 - accuracy: 1.0000 - val_loss: 3.2831 - val_accuracy: 0.6500\n",
            "Epoch 58/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 4.9858e-06 - accuracy: 1.0000 - val_loss: 3.2810 - val_accuracy: 0.6500\n",
            "Epoch 59/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 4.9187e-06 - accuracy: 1.0000 - val_loss: 3.2789 - val_accuracy: 0.6500\n",
            "Epoch 60/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.8383e-06 - accuracy: 1.0000 - val_loss: 3.2770 - val_accuracy: 0.6500\n",
            "Epoch 61/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 4.7519e-06 - accuracy: 1.0000 - val_loss: 3.2753 - val_accuracy: 0.6500\n",
            "Epoch 62/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 4.6923e-06 - accuracy: 1.0000 - val_loss: 3.2736 - val_accuracy: 0.6500\n",
            "Epoch 63/100\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 4.6252e-06 - accuracy: 1.0000 - val_loss: 3.2719 - val_accuracy: 0.6500\n",
            "Epoch 64/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 4.5596e-06 - accuracy: 1.0000 - val_loss: 3.2705 - val_accuracy: 0.6500\n",
            "Epoch 65/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.5000e-06 - accuracy: 1.0000 - val_loss: 3.2691 - val_accuracy: 0.6500\n",
            "Epoch 66/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.4360e-06 - accuracy: 1.0000 - val_loss: 3.2677 - val_accuracy: 0.6500\n",
            "Epoch 67/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 4.3928e-06 - accuracy: 1.0000 - val_loss: 3.2663 - val_accuracy: 0.6500\n",
            "Epoch 68/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.3332e-06 - accuracy: 1.0000 - val_loss: 3.2650 - val_accuracy: 0.6500\n",
            "Epoch 69/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 4.2855e-06 - accuracy: 1.0000 - val_loss: 3.2637 - val_accuracy: 0.6500\n",
            "Epoch 70/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.2303e-06 - accuracy: 1.0000 - val_loss: 3.2624 - val_accuracy: 0.6500\n",
            "Epoch 71/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 4.1842e-06 - accuracy: 1.0000 - val_loss: 3.2611 - val_accuracy: 0.6500\n",
            "Epoch 72/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.1409e-06 - accuracy: 1.0000 - val_loss: 3.2598 - val_accuracy: 0.6500\n",
            "Epoch 73/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.0888e-06 - accuracy: 1.0000 - val_loss: 3.2585 - val_accuracy: 0.6500\n",
            "Epoch 74/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 4.0471e-06 - accuracy: 1.0000 - val_loss: 3.2573 - val_accuracy: 0.6500\n",
            "Epoch 75/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.9979e-06 - accuracy: 1.0000 - val_loss: 3.2562 - val_accuracy: 0.6500\n",
            "Epoch 76/100\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 3.9577e-06 - accuracy: 1.0000 - val_loss: 3.2549 - val_accuracy: 0.6500\n",
            "Epoch 77/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.9145e-06 - accuracy: 1.0000 - val_loss: 3.2536 - val_accuracy: 0.6500\n",
            "Epoch 78/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 3.8698e-06 - accuracy: 1.0000 - val_loss: 3.2525 - val_accuracy: 0.6500\n",
            "Epoch 79/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.8206e-06 - accuracy: 1.0000 - val_loss: 3.2514 - val_accuracy: 0.6500\n",
            "Epoch 80/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.7848e-06 - accuracy: 1.0000 - val_loss: 3.2501 - val_accuracy: 0.6500\n",
            "Epoch 81/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.7401e-06 - accuracy: 1.0000 - val_loss: 3.2489 - val_accuracy: 0.6500\n",
            "Epoch 82/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.6999e-06 - accuracy: 1.0000 - val_loss: 3.2477 - val_accuracy: 0.6500\n",
            "Epoch 83/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.6611e-06 - accuracy: 1.0000 - val_loss: 3.2465 - val_accuracy: 0.6500\n",
            "Epoch 84/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.6224e-06 - accuracy: 1.0000 - val_loss: 3.2454 - val_accuracy: 0.6500\n",
            "Epoch 85/100\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 3.5852e-06 - accuracy: 1.0000 - val_loss: 3.2443 - val_accuracy: 0.6500\n",
            "Epoch 86/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 3.5479e-06 - accuracy: 1.0000 - val_loss: 3.2431 - val_accuracy: 0.6500\n",
            "Epoch 87/100\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 3.5092e-06 - accuracy: 1.0000 - val_loss: 3.2419 - val_accuracy: 0.6500\n",
            "Epoch 88/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 3.4764e-06 - accuracy: 1.0000 - val_loss: 3.2407 - val_accuracy: 0.6500\n",
            "Epoch 89/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.4332e-06 - accuracy: 1.0000 - val_loss: 3.2394 - val_accuracy: 0.6500\n",
            "Epoch 90/100\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 3.4019e-06 - accuracy: 1.0000 - val_loss: 3.2381 - val_accuracy: 0.6500\n",
            "Epoch 91/100\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 3.3691e-06 - accuracy: 1.0000 - val_loss: 3.2368 - val_accuracy: 0.6500\n",
            "Epoch 92/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.3244e-06 - accuracy: 1.0000 - val_loss: 3.2356 - val_accuracy: 0.6500\n",
            "Epoch 93/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.2901e-06 - accuracy: 1.0000 - val_loss: 3.2344 - val_accuracy: 0.6500\n",
            "Epoch 94/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.2573e-06 - accuracy: 1.0000 - val_loss: 3.2333 - val_accuracy: 0.6500\n",
            "Epoch 95/100\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 3.2216e-06 - accuracy: 1.0000 - val_loss: 3.2322 - val_accuracy: 0.6500\n",
            "Epoch 96/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.1933e-06 - accuracy: 1.0000 - val_loss: 3.2310 - val_accuracy: 0.6500\n",
            "Epoch 97/100\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 3.1590e-06 - accuracy: 1.0000 - val_loss: 3.2298 - val_accuracy: 0.6500\n",
            "Epoch 98/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.1262e-06 - accuracy: 1.0000 - val_loss: 3.2286 - val_accuracy: 0.6500\n",
            "Epoch 99/100\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 3.0979e-06 - accuracy: 1.0000 - val_loss: 3.2276 - val_accuracy: 0.6500\n",
            "Epoch 100/100\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 3.0651e-06 - accuracy: 1.0000 - val_loss: 3.2265 - val_accuracy: 0.6500\n",
            "(800, 28, 28, 1) (200, 28, 28, 1)\n",
            "Epoch 1/100\n",
            "13/13 [==============================] - 1s 27ms/step - loss: 2.0397 - accuracy: 0.5987 - val_loss: 0.7795 - val_accuracy: 0.7700\n",
            "Epoch 2/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.5443 - accuracy: 0.8462 - val_loss: 0.6392 - val_accuracy: 0.7950\n",
            "Epoch 3/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.2291 - accuracy: 0.9200 - val_loss: 0.5658 - val_accuracy: 0.8350\n",
            "Epoch 4/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0858 - accuracy: 0.9688 - val_loss: 0.6050 - val_accuracy: 0.8400\n",
            "Epoch 5/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0324 - accuracy: 0.9950 - val_loss: 0.5901 - val_accuracy: 0.8350\n",
            "Epoch 6/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0175 - accuracy: 0.9987 - val_loss: 0.6316 - val_accuracy: 0.8400\n",
            "Epoch 7/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.5897 - val_accuracy: 0.8400\n",
            "Epoch 8/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.6481 - val_accuracy: 0.8350\n",
            "Epoch 9/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6070 - val_accuracy: 0.8350\n",
            "Epoch 10/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.6330 - val_accuracy: 0.8350\n",
            "Epoch 11/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.6442 - val_accuracy: 0.8250\n",
            "Epoch 12/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.6388 - val_accuracy: 0.8250\n",
            "Epoch 13/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.6519 - val_accuracy: 0.8250\n",
            "Epoch 14/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.6465 - val_accuracy: 0.8250\n",
            "Epoch 15/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.6484 - val_accuracy: 0.8250\n",
            "Epoch 16/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.6577 - val_accuracy: 0.8250\n",
            "Epoch 17/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.6626 - val_accuracy: 0.8300\n",
            "Epoch 18/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.6701 - val_accuracy: 0.8250\n",
            "Epoch 19/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.6742 - val_accuracy: 0.8250\n",
            "Epoch 20/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6745 - val_accuracy: 0.8300\n",
            "Epoch 21/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.6872 - val_accuracy: 0.8250\n",
            "Epoch 22/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.6833 - val_accuracy: 0.8300\n",
            "Epoch 23/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.6862 - val_accuracy: 0.8300\n",
            "Epoch 24/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.6908 - val_accuracy: 0.8300\n",
            "Epoch 25/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.7026 - val_accuracy: 0.8300\n",
            "Epoch 26/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.6953 - val_accuracy: 0.8300\n",
            "Epoch 27/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7026 - val_accuracy: 0.8300\n",
            "Epoch 28/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.7080 - val_accuracy: 0.8300\n",
            "Epoch 29/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.7079 - val_accuracy: 0.8300\n",
            "Epoch 30/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 9.6720e-04 - accuracy: 1.0000 - val_loss: 0.7091 - val_accuracy: 0.8300\n",
            "Epoch 31/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 9.1939e-04 - accuracy: 1.0000 - val_loss: 0.7170 - val_accuracy: 0.8300\n",
            "Epoch 32/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 8.7203e-04 - accuracy: 1.0000 - val_loss: 0.7192 - val_accuracy: 0.8300\n",
            "Epoch 33/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 8.2833e-04 - accuracy: 1.0000 - val_loss: 0.7247 - val_accuracy: 0.8300\n",
            "Epoch 34/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 7.8993e-04 - accuracy: 1.0000 - val_loss: 0.7223 - val_accuracy: 0.8350\n",
            "Epoch 35/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 7.5197e-04 - accuracy: 1.0000 - val_loss: 0.7246 - val_accuracy: 0.8350\n",
            "Epoch 36/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 7.2399e-04 - accuracy: 1.0000 - val_loss: 0.7329 - val_accuracy: 0.8350\n",
            "Epoch 37/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 6.8595e-04 - accuracy: 1.0000 - val_loss: 0.7357 - val_accuracy: 0.8300\n",
            "Epoch 38/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 6.6347e-04 - accuracy: 1.0000 - val_loss: 0.7272 - val_accuracy: 0.8300\n",
            "Epoch 39/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 6.2812e-04 - accuracy: 1.0000 - val_loss: 0.7399 - val_accuracy: 0.8300\n",
            "Epoch 40/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 6.0399e-04 - accuracy: 1.0000 - val_loss: 0.7426 - val_accuracy: 0.8350\n",
            "Epoch 41/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 5.7517e-04 - accuracy: 1.0000 - val_loss: 0.7405 - val_accuracy: 0.8300\n",
            "Epoch 42/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 5.5130e-04 - accuracy: 1.0000 - val_loss: 0.7406 - val_accuracy: 0.8300\n",
            "Epoch 43/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 5.2942e-04 - accuracy: 1.0000 - val_loss: 0.7503 - val_accuracy: 0.8300\n",
            "Epoch 44/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 5.1021e-04 - accuracy: 1.0000 - val_loss: 0.7531 - val_accuracy: 0.8300\n",
            "Epoch 45/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 4.9028e-04 - accuracy: 1.0000 - val_loss: 0.7513 - val_accuracy: 0.8300\n",
            "Epoch 46/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 4.7174e-04 - accuracy: 1.0000 - val_loss: 0.7538 - val_accuracy: 0.8300\n",
            "Epoch 47/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 4.5601e-04 - accuracy: 1.0000 - val_loss: 0.7568 - val_accuracy: 0.8300\n",
            "Epoch 48/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 4.4572e-04 - accuracy: 1.0000 - val_loss: 0.7528 - val_accuracy: 0.8350\n",
            "Epoch 49/100\n",
            "13/13 [==============================] - 0s 8ms/step - loss: 4.2686e-04 - accuracy: 1.0000 - val_loss: 0.7590 - val_accuracy: 0.8300\n",
            "Epoch 50/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 4.1157e-04 - accuracy: 1.0000 - val_loss: 0.7625 - val_accuracy: 0.8300\n",
            "Epoch 51/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.9891e-04 - accuracy: 1.0000 - val_loss: 0.7642 - val_accuracy: 0.8350\n",
            "Epoch 52/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.8366e-04 - accuracy: 1.0000 - val_loss: 0.7670 - val_accuracy: 0.8350\n",
            "Epoch 53/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.7212e-04 - accuracy: 1.0000 - val_loss: 0.7679 - val_accuracy: 0.8350\n",
            "Epoch 54/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.6107e-04 - accuracy: 1.0000 - val_loss: 0.7675 - val_accuracy: 0.8350\n",
            "Epoch 55/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.4933e-04 - accuracy: 1.0000 - val_loss: 0.7701 - val_accuracy: 0.8350\n",
            "Epoch 56/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.3879e-04 - accuracy: 1.0000 - val_loss: 0.7695 - val_accuracy: 0.8350\n",
            "Epoch 57/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.2794e-04 - accuracy: 1.0000 - val_loss: 0.7699 - val_accuracy: 0.8350\n",
            "Epoch 58/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.1901e-04 - accuracy: 1.0000 - val_loss: 0.7734 - val_accuracy: 0.8350\n",
            "Epoch 59/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 3.0905e-04 - accuracy: 1.0000 - val_loss: 0.7748 - val_accuracy: 0.8350\n",
            "Epoch 60/100\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 3.0064e-04 - accuracy: 1.0000 - val_loss: 0.7761 - val_accuracy: 0.8350\n",
            "Epoch 61/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.9268e-04 - accuracy: 1.0000 - val_loss: 0.7815 - val_accuracy: 0.8350\n",
            "Epoch 62/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.8361e-04 - accuracy: 1.0000 - val_loss: 0.7814 - val_accuracy: 0.8350\n",
            "Epoch 63/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.7561e-04 - accuracy: 1.0000 - val_loss: 0.7835 - val_accuracy: 0.8350\n",
            "Epoch 64/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.6818e-04 - accuracy: 1.0000 - val_loss: 0.7828 - val_accuracy: 0.8300\n",
            "Epoch 65/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.6077e-04 - accuracy: 1.0000 - val_loss: 0.7838 - val_accuracy: 0.8300\n",
            "Epoch 66/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.5411e-04 - accuracy: 1.0000 - val_loss: 0.7855 - val_accuracy: 0.8300\n",
            "Epoch 67/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.4739e-04 - accuracy: 1.0000 - val_loss: 0.7884 - val_accuracy: 0.8350\n",
            "Epoch 68/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 2.4029e-04 - accuracy: 1.0000 - val_loss: 0.7903 - val_accuracy: 0.8300\n",
            "Epoch 69/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.3431e-04 - accuracy: 1.0000 - val_loss: 0.7906 - val_accuracy: 0.8300\n",
            "Epoch 70/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.2806e-04 - accuracy: 1.0000 - val_loss: 0.7913 - val_accuracy: 0.8300\n",
            "Epoch 71/100\n",
            "13/13 [==============================] - 0s 13ms/step - loss: 2.2253e-04 - accuracy: 1.0000 - val_loss: 0.7945 - val_accuracy: 0.8300\n",
            "Epoch 72/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.1644e-04 - accuracy: 1.0000 - val_loss: 0.7958 - val_accuracy: 0.8300\n",
            "Epoch 73/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 2.1027e-04 - accuracy: 1.0000 - val_loss: 0.7963 - val_accuracy: 0.8300\n",
            "Epoch 74/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 2.0440e-04 - accuracy: 1.0000 - val_loss: 0.7989 - val_accuracy: 0.8300\n",
            "Epoch 75/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.9927e-04 - accuracy: 1.0000 - val_loss: 0.8010 - val_accuracy: 0.8300\n",
            "Epoch 76/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 1.9432e-04 - accuracy: 1.0000 - val_loss: 0.7994 - val_accuracy: 0.8300\n",
            "Epoch 77/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.8907e-04 - accuracy: 1.0000 - val_loss: 0.8033 - val_accuracy: 0.8300\n",
            "Epoch 78/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.8466e-04 - accuracy: 1.0000 - val_loss: 0.8042 - val_accuracy: 0.8300\n",
            "Epoch 79/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.8046e-04 - accuracy: 1.0000 - val_loss: 0.8016 - val_accuracy: 0.8300\n",
            "Epoch 80/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.7614e-04 - accuracy: 1.0000 - val_loss: 0.8086 - val_accuracy: 0.8300\n",
            "Epoch 81/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.7119e-04 - accuracy: 1.0000 - val_loss: 0.8060 - val_accuracy: 0.8300\n",
            "Epoch 82/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.6671e-04 - accuracy: 1.0000 - val_loss: 0.8118 - val_accuracy: 0.8300\n",
            "Epoch 83/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.6311e-04 - accuracy: 1.0000 - val_loss: 0.8095 - val_accuracy: 0.8300\n",
            "Epoch 84/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.5880e-04 - accuracy: 1.0000 - val_loss: 0.8123 - val_accuracy: 0.8300\n",
            "Epoch 85/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.5521e-04 - accuracy: 1.0000 - val_loss: 0.8131 - val_accuracy: 0.8300\n",
            "Epoch 86/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.5136e-04 - accuracy: 1.0000 - val_loss: 0.8117 - val_accuracy: 0.8300\n",
            "Epoch 87/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.4789e-04 - accuracy: 1.0000 - val_loss: 0.8159 - val_accuracy: 0.8300\n",
            "Epoch 88/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.4498e-04 - accuracy: 1.0000 - val_loss: 0.8172 - val_accuracy: 0.8300\n",
            "Epoch 89/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.4144e-04 - accuracy: 1.0000 - val_loss: 0.8148 - val_accuracy: 0.8300\n",
            "Epoch 90/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.3793e-04 - accuracy: 1.0000 - val_loss: 0.8172 - val_accuracy: 0.8300\n",
            "Epoch 91/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 1.3504e-04 - accuracy: 1.0000 - val_loss: 0.8217 - val_accuracy: 0.8300\n",
            "Epoch 92/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.3216e-04 - accuracy: 1.0000 - val_loss: 0.8192 - val_accuracy: 0.8300\n",
            "Epoch 93/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.2897e-04 - accuracy: 1.0000 - val_loss: 0.8223 - val_accuracy: 0.8300\n",
            "Epoch 94/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 1.2604e-04 - accuracy: 1.0000 - val_loss: 0.8235 - val_accuracy: 0.8300\n",
            "Epoch 95/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.2336e-04 - accuracy: 1.0000 - val_loss: 0.8264 - val_accuracy: 0.8300\n",
            "Epoch 96/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.2083e-04 - accuracy: 1.0000 - val_loss: 0.8256 - val_accuracy: 0.8300\n",
            "Epoch 97/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.1827e-04 - accuracy: 1.0000 - val_loss: 0.8264 - val_accuracy: 0.8300\n",
            "Epoch 98/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.1613e-04 - accuracy: 1.0000 - val_loss: 0.8281 - val_accuracy: 0.8300\n",
            "Epoch 99/100\n",
            "13/13 [==============================] - 0s 10ms/step - loss: 1.1386e-04 - accuracy: 1.0000 - val_loss: 0.8274 - val_accuracy: 0.8300\n",
            "Epoch 100/100\n",
            "13/13 [==============================] - 0s 9ms/step - loss: 1.1112e-04 - accuracy: 1.0000 - val_loss: 0.8282 - val_accuracy: 0.8300\n",
            "(8000, 28, 28, 1) (2000, 28, 28, 1)\n",
            "Epoch 1/100\n",
            "125/125 [==============================] - 2s 8ms/step - loss: 0.7049 - accuracy: 0.7995 - val_loss: 0.3917 - val_accuracy: 0.8535\n",
            "Epoch 2/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.2688 - accuracy: 0.9024 - val_loss: 0.3474 - val_accuracy: 0.8840\n",
            "Epoch 3/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1778 - accuracy: 0.9346 - val_loss: 0.3467 - val_accuracy: 0.8800\n",
            "Epoch 4/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.1343 - accuracy: 0.9530 - val_loss: 0.3410 - val_accuracy: 0.8860\n",
            "Epoch 5/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0852 - accuracy: 0.9695 - val_loss: 0.3479 - val_accuracy: 0.8895\n",
            "Epoch 6/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0636 - accuracy: 0.9800 - val_loss: 0.4209 - val_accuracy: 0.8885\n",
            "Epoch 7/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0491 - accuracy: 0.9827 - val_loss: 0.4025 - val_accuracy: 0.8940\n",
            "Epoch 8/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0280 - accuracy: 0.9916 - val_loss: 0.3861 - val_accuracy: 0.8950\n",
            "Epoch 9/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0204 - accuracy: 0.9948 - val_loss: 0.4025 - val_accuracy: 0.8965\n",
            "Epoch 10/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0150 - accuracy: 0.9974 - val_loss: 0.4145 - val_accuracy: 0.9030\n",
            "Epoch 11/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0397 - accuracy: 0.9866 - val_loss: 0.4747 - val_accuracy: 0.8890\n",
            "Epoch 12/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0330 - accuracy: 0.9877 - val_loss: 0.4856 - val_accuracy: 0.8885\n",
            "Epoch 13/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0346 - accuracy: 0.9871 - val_loss: 0.6124 - val_accuracy: 0.8720\n",
            "Epoch 14/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0773 - accuracy: 0.9755 - val_loss: 0.6118 - val_accuracy: 0.8680\n",
            "Epoch 15/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0499 - accuracy: 0.9854 - val_loss: 0.4902 - val_accuracy: 0.8970\n",
            "Epoch 16/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0128 - accuracy: 0.9961 - val_loss: 0.5363 - val_accuracy: 0.8940\n",
            "Epoch 17/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0145 - accuracy: 0.9966 - val_loss: 0.5407 - val_accuracy: 0.8970\n",
            "Epoch 18/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0050 - accuracy: 0.9990 - val_loss: 0.5633 - val_accuracy: 0.8920\n",
            "Epoch 19/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.5441 - val_accuracy: 0.8980\n",
            "Epoch 20/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 7.0248e-04 - accuracy: 1.0000 - val_loss: 0.5505 - val_accuracy: 0.8980\n",
            "Epoch 21/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 5.5815e-04 - accuracy: 1.0000 - val_loss: 0.5527 - val_accuracy: 0.8990\n",
            "Epoch 22/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.8093e-04 - accuracy: 1.0000 - val_loss: 0.5559 - val_accuracy: 0.8980\n",
            "Epoch 23/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.2379e-04 - accuracy: 1.0000 - val_loss: 0.5590 - val_accuracy: 0.8995\n",
            "Epoch 24/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.8239e-04 - accuracy: 1.0000 - val_loss: 0.5616 - val_accuracy: 0.8995\n",
            "Epoch 25/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.4602e-04 - accuracy: 1.0000 - val_loss: 0.5644 - val_accuracy: 0.8985\n",
            "Epoch 26/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.1682e-04 - accuracy: 1.0000 - val_loss: 0.5677 - val_accuracy: 0.8980\n",
            "Epoch 27/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.8988e-04 - accuracy: 1.0000 - val_loss: 0.5717 - val_accuracy: 0.8985\n",
            "Epoch 28/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.6575e-04 - accuracy: 1.0000 - val_loss: 0.5730 - val_accuracy: 0.8995\n",
            "Epoch 29/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.4751e-04 - accuracy: 1.0000 - val_loss: 0.5777 - val_accuracy: 0.8985\n",
            "Epoch 30/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.2803e-04 - accuracy: 1.0000 - val_loss: 0.5806 - val_accuracy: 0.8990\n",
            "Epoch 31/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.1049e-04 - accuracy: 1.0000 - val_loss: 0.5796 - val_accuracy: 0.9000\n",
            "Epoch 32/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.9611e-04 - accuracy: 1.0000 - val_loss: 0.5827 - val_accuracy: 0.8985\n",
            "Epoch 33/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.8410e-04 - accuracy: 1.0000 - val_loss: 0.5860 - val_accuracy: 0.8985\n",
            "Epoch 34/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.7136e-04 - accuracy: 1.0000 - val_loss: 0.5898 - val_accuracy: 0.8990\n",
            "Epoch 35/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.5920e-04 - accuracy: 1.0000 - val_loss: 0.5909 - val_accuracy: 0.8995\n",
            "Epoch 36/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.4796e-04 - accuracy: 1.0000 - val_loss: 0.5944 - val_accuracy: 0.8985\n",
            "Epoch 37/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.3921e-04 - accuracy: 1.0000 - val_loss: 0.5966 - val_accuracy: 0.8980\n",
            "Epoch 38/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.2892e-04 - accuracy: 1.0000 - val_loss: 0.5992 - val_accuracy: 0.8985\n",
            "Epoch 39/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.2188e-04 - accuracy: 1.0000 - val_loss: 0.6025 - val_accuracy: 0.8975\n",
            "Epoch 40/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.1376e-04 - accuracy: 1.0000 - val_loss: 0.6053 - val_accuracy: 0.8995\n",
            "Epoch 41/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0654e-04 - accuracy: 1.0000 - val_loss: 0.6080 - val_accuracy: 0.8980\n",
            "Epoch 42/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 9.9724e-05 - accuracy: 1.0000 - val_loss: 0.6097 - val_accuracy: 0.8985\n",
            "Epoch 43/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 9.4102e-05 - accuracy: 1.0000 - val_loss: 0.6131 - val_accuracy: 0.8980\n",
            "Epoch 44/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 8.7486e-05 - accuracy: 1.0000 - val_loss: 0.6149 - val_accuracy: 0.8975\n",
            "Epoch 45/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 8.2483e-05 - accuracy: 1.0000 - val_loss: 0.6172 - val_accuracy: 0.8975\n",
            "Epoch 46/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 7.7129e-05 - accuracy: 1.0000 - val_loss: 0.6198 - val_accuracy: 0.8985\n",
            "Epoch 47/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 7.2303e-05 - accuracy: 1.0000 - val_loss: 0.6234 - val_accuracy: 0.8975\n",
            "Epoch 48/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 6.7619e-05 - accuracy: 1.0000 - val_loss: 0.6248 - val_accuracy: 0.8980\n",
            "Epoch 49/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 6.3728e-05 - accuracy: 1.0000 - val_loss: 0.6267 - val_accuracy: 0.8970\n",
            "Epoch 50/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 5.9798e-05 - accuracy: 1.0000 - val_loss: 0.6303 - val_accuracy: 0.8980\n",
            "Epoch 51/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 5.6338e-05 - accuracy: 1.0000 - val_loss: 0.6301 - val_accuracy: 0.8985\n",
            "Epoch 52/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 5.2750e-05 - accuracy: 1.0000 - val_loss: 0.6339 - val_accuracy: 0.8980\n",
            "Epoch 53/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.9369e-05 - accuracy: 1.0000 - val_loss: 0.6372 - val_accuracy: 0.8975\n",
            "Epoch 54/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.6364e-05 - accuracy: 1.0000 - val_loss: 0.6409 - val_accuracy: 0.8965\n",
            "Epoch 55/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.3452e-05 - accuracy: 1.0000 - val_loss: 0.6405 - val_accuracy: 0.8960\n",
            "Epoch 56/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.0926e-05 - accuracy: 1.0000 - val_loss: 0.6441 - val_accuracy: 0.8965\n",
            "Epoch 57/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.8487e-05 - accuracy: 1.0000 - val_loss: 0.6459 - val_accuracy: 0.8965\n",
            "Epoch 58/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.6087e-05 - accuracy: 1.0000 - val_loss: 0.6489 - val_accuracy: 0.8975\n",
            "Epoch 59/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.4001e-05 - accuracy: 1.0000 - val_loss: 0.6506 - val_accuracy: 0.8975\n",
            "Epoch 60/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.1737e-05 - accuracy: 1.0000 - val_loss: 0.6549 - val_accuracy: 0.8980\n",
            "Epoch 61/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.9738e-05 - accuracy: 1.0000 - val_loss: 0.6574 - val_accuracy: 0.8960\n",
            "Epoch 62/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.7833e-05 - accuracy: 1.0000 - val_loss: 0.6616 - val_accuracy: 0.8975\n",
            "Epoch 63/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.6289e-05 - accuracy: 1.0000 - val_loss: 0.6619 - val_accuracy: 0.8980\n",
            "Epoch 64/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.4652e-05 - accuracy: 1.0000 - val_loss: 0.6641 - val_accuracy: 0.8985\n",
            "Epoch 65/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.2976e-05 - accuracy: 1.0000 - val_loss: 0.6684 - val_accuracy: 0.8980\n",
            "Epoch 66/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.1609e-05 - accuracy: 1.0000 - val_loss: 0.6706 - val_accuracy: 0.8960\n",
            "Epoch 67/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.0320e-05 - accuracy: 1.0000 - val_loss: 0.6717 - val_accuracy: 0.8970\n",
            "Epoch 68/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.9013e-05 - accuracy: 1.0000 - val_loss: 0.6737 - val_accuracy: 0.8970\n",
            "Epoch 69/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.7966e-05 - accuracy: 1.0000 - val_loss: 0.6772 - val_accuracy: 0.8990\n",
            "Epoch 70/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.6752e-05 - accuracy: 1.0000 - val_loss: 0.6808 - val_accuracy: 0.8980\n",
            "Epoch 71/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.5733e-05 - accuracy: 1.0000 - val_loss: 0.6823 - val_accuracy: 0.8975\n",
            "Epoch 72/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.4773e-05 - accuracy: 1.0000 - val_loss: 0.6846 - val_accuracy: 0.8975\n",
            "Epoch 73/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.3790e-05 - accuracy: 1.0000 - val_loss: 0.6878 - val_accuracy: 0.8965\n",
            "Epoch 74/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.2907e-05 - accuracy: 1.0000 - val_loss: 0.6915 - val_accuracy: 0.8970\n",
            "Epoch 75/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.2154e-05 - accuracy: 1.0000 - val_loss: 0.6917 - val_accuracy: 0.8980\n",
            "Epoch 76/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.1389e-05 - accuracy: 1.0000 - val_loss: 0.6953 - val_accuracy: 0.8985\n",
            "Epoch 77/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 1.0600e-05 - accuracy: 1.0000 - val_loss: 0.6996 - val_accuracy: 0.8975\n",
            "Epoch 78/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 9.9843e-06 - accuracy: 1.0000 - val_loss: 0.7020 - val_accuracy: 0.8990\n",
            "Epoch 79/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 9.4388e-06 - accuracy: 1.0000 - val_loss: 0.7047 - val_accuracy: 0.8990\n",
            "Epoch 80/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 8.8509e-06 - accuracy: 1.0000 - val_loss: 0.7058 - val_accuracy: 0.8980\n",
            "Epoch 81/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 8.2646e-06 - accuracy: 1.0000 - val_loss: 0.7091 - val_accuracy: 0.8990\n",
            "Epoch 82/100\n",
            "125/125 [==============================] - 1s 8ms/step - loss: 7.7058e-06 - accuracy: 1.0000 - val_loss: 0.7124 - val_accuracy: 0.8980\n",
            "Epoch 83/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 7.2355e-06 - accuracy: 1.0000 - val_loss: 0.7156 - val_accuracy: 0.8980\n",
            "Epoch 84/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 6.7624e-06 - accuracy: 1.0000 - val_loss: 0.7172 - val_accuracy: 0.9005\n",
            "Epoch 85/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 6.3733e-06 - accuracy: 1.0000 - val_loss: 0.7222 - val_accuracy: 0.8990\n",
            "Epoch 86/100\n",
            "125/125 [==============================] - 1s 7ms/step - loss: 5.9348e-06 - accuracy: 1.0000 - val_loss: 0.7233 - val_accuracy: 0.8985\n",
            "Epoch 87/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 5.5520e-06 - accuracy: 1.0000 - val_loss: 0.7256 - val_accuracy: 0.8990\n",
            "Epoch 88/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 5.1818e-06 - accuracy: 1.0000 - val_loss: 0.7292 - val_accuracy: 0.8980\n",
            "Epoch 89/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.8329e-06 - accuracy: 1.0000 - val_loss: 0.7304 - val_accuracy: 0.8990\n",
            "Epoch 90/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.5482e-06 - accuracy: 1.0000 - val_loss: 0.7340 - val_accuracy: 0.8985\n",
            "Epoch 91/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 4.2365e-06 - accuracy: 1.0000 - val_loss: 0.7362 - val_accuracy: 0.9000\n",
            "Epoch 92/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.9717e-06 - accuracy: 1.0000 - val_loss: 0.7392 - val_accuracy: 0.8985\n",
            "Epoch 93/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.7309e-06 - accuracy: 1.0000 - val_loss: 0.7414 - val_accuracy: 0.8990\n",
            "Epoch 94/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.5009e-06 - accuracy: 1.0000 - val_loss: 0.7446 - val_accuracy: 0.8995\n",
            "Epoch 95/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.2930e-06 - accuracy: 1.0000 - val_loss: 0.7459 - val_accuracy: 0.9000\n",
            "Epoch 96/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 3.0680e-06 - accuracy: 1.0000 - val_loss: 0.7492 - val_accuracy: 0.8985\n",
            "Epoch 97/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.8612e-06 - accuracy: 1.0000 - val_loss: 0.7519 - val_accuracy: 0.9000\n",
            "Epoch 98/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.6746e-06 - accuracy: 1.0000 - val_loss: 0.7569 - val_accuracy: 0.9000\n",
            "Epoch 99/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.5060e-06 - accuracy: 1.0000 - val_loss: 0.7598 - val_accuracy: 0.9005\n",
            "Epoch 100/100\n",
            "125/125 [==============================] - 1s 6ms/step - loss: 2.3495e-06 - accuracy: 1.0000 - val_loss: 0.7607 - val_accuracy: 0.9010\n",
            "(24000, 28, 28, 1) (6000, 28, 28, 1)\n",
            "Epoch 1/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.4455 - accuracy: 0.8507 - val_loss: 0.3540 - val_accuracy: 0.8807\n",
            "Epoch 2/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.2505 - accuracy: 0.9060 - val_loss: 0.3328 - val_accuracy: 0.8847\n",
            "Epoch 3/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1895 - accuracy: 0.9285 - val_loss: 0.3123 - val_accuracy: 0.8983\n",
            "Epoch 4/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1557 - accuracy: 0.9417 - val_loss: 0.3585 - val_accuracy: 0.8872\n",
            "Epoch 5/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1307 - accuracy: 0.9500 - val_loss: 0.3786 - val_accuracy: 0.8903\n",
            "Epoch 6/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.1084 - accuracy: 0.9594 - val_loss: 0.3702 - val_accuracy: 0.9017\n",
            "Epoch 7/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.1020 - accuracy: 0.9607 - val_loss: 0.3831 - val_accuracy: 0.9030\n",
            "Epoch 8/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0798 - accuracy: 0.9707 - val_loss: 0.4556 - val_accuracy: 0.8930\n",
            "Epoch 9/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0661 - accuracy: 0.9762 - val_loss: 0.5055 - val_accuracy: 0.8872\n",
            "Epoch 10/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0726 - accuracy: 0.9723 - val_loss: 0.5179 - val_accuracy: 0.8945\n",
            "Epoch 11/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0685 - accuracy: 0.9758 - val_loss: 0.5118 - val_accuracy: 0.8950\n",
            "Epoch 12/100\n",
            "375/375 [==============================] - 3s 8ms/step - loss: 0.0581 - accuracy: 0.9787 - val_loss: 0.5696 - val_accuracy: 0.8868\n",
            "Epoch 13/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0551 - accuracy: 0.9805 - val_loss: 0.5644 - val_accuracy: 0.8962\n",
            "Epoch 14/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0454 - accuracy: 0.9830 - val_loss: 0.6063 - val_accuracy: 0.9007\n",
            "Epoch 15/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0566 - accuracy: 0.9799 - val_loss: 0.5716 - val_accuracy: 0.9003\n",
            "Epoch 16/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0291 - accuracy: 0.9897 - val_loss: 0.5924 - val_accuracy: 0.9053\n",
            "Epoch 17/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0231 - accuracy: 0.9919 - val_loss: 0.7331 - val_accuracy: 0.8905\n",
            "Epoch 18/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0523 - accuracy: 0.9823 - val_loss: 0.6886 - val_accuracy: 0.8887\n",
            "Epoch 19/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0520 - accuracy: 0.9822 - val_loss: 0.6989 - val_accuracy: 0.9000\n",
            "Epoch 20/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0253 - accuracy: 0.9913 - val_loss: 0.7321 - val_accuracy: 0.8978\n",
            "Epoch 21/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0314 - accuracy: 0.9892 - val_loss: 0.7302 - val_accuracy: 0.8950\n",
            "Epoch 22/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0345 - accuracy: 0.9872 - val_loss: 0.7704 - val_accuracy: 0.8998\n",
            "Epoch 23/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0292 - accuracy: 0.9899 - val_loss: 0.7636 - val_accuracy: 0.8972\n",
            "Epoch 24/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0202 - accuracy: 0.9930 - val_loss: 0.6866 - val_accuracy: 0.9043\n",
            "Epoch 25/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0252 - accuracy: 0.9918 - val_loss: 0.7787 - val_accuracy: 0.9007\n",
            "Epoch 26/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0317 - accuracy: 0.9902 - val_loss: 0.8635 - val_accuracy: 0.8938\n",
            "Epoch 27/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0296 - accuracy: 0.9906 - val_loss: 0.7607 - val_accuracy: 0.9045\n",
            "Epoch 28/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0172 - accuracy: 0.9936 - val_loss: 0.8022 - val_accuracy: 0.9035\n",
            "Epoch 29/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0133 - accuracy: 0.9956 - val_loss: 0.7898 - val_accuracy: 0.9068\n",
            "Epoch 30/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0084 - accuracy: 0.9974 - val_loss: 0.7843 - val_accuracy: 0.9068\n",
            "Epoch 31/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0537 - accuracy: 0.9843 - val_loss: 0.8477 - val_accuracy: 0.8950\n",
            "Epoch 32/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0323 - accuracy: 0.9890 - val_loss: 0.8903 - val_accuracy: 0.8972\n",
            "Epoch 33/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0177 - accuracy: 0.9944 - val_loss: 0.8448 - val_accuracy: 0.9033\n",
            "Epoch 34/100\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0150 - accuracy: 0.9949 - val_loss: 0.8663 - val_accuracy: 0.9057\n",
            "Epoch 35/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0204 - accuracy: 0.9937 - val_loss: 0.9002 - val_accuracy: 0.8967\n",
            "Epoch 36/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0234 - accuracy: 0.9921 - val_loss: 0.9251 - val_accuracy: 0.8977\n",
            "Epoch 37/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0324 - accuracy: 0.9899 - val_loss: 0.8985 - val_accuracy: 0.9005\n",
            "Epoch 38/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0158 - accuracy: 0.9952 - val_loss: 0.9438 - val_accuracy: 0.8978\n",
            "Epoch 39/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0164 - accuracy: 0.9943 - val_loss: 0.9279 - val_accuracy: 0.8980\n",
            "Epoch 40/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0243 - accuracy: 0.9926 - val_loss: 0.9804 - val_accuracy: 0.8973\n",
            "Epoch 41/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0241 - accuracy: 0.9930 - val_loss: 0.9415 - val_accuracy: 0.9002\n",
            "Epoch 42/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0147 - accuracy: 0.9949 - val_loss: 0.9606 - val_accuracy: 0.9022\n",
            "Epoch 43/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0163 - accuracy: 0.9950 - val_loss: 1.0691 - val_accuracy: 0.9000\n",
            "Epoch 44/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0311 - accuracy: 0.9907 - val_loss: 0.9762 - val_accuracy: 0.9052\n",
            "Epoch 45/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 0.0087 - accuracy: 0.9973 - val_loss: 0.9713 - val_accuracy: 0.9048\n",
            "Epoch 46/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0160 - accuracy: 0.9958 - val_loss: 1.0117 - val_accuracy: 0.9043\n",
            "Epoch 47/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0245 - accuracy: 0.9929 - val_loss: 0.9781 - val_accuracy: 0.9023\n",
            "Epoch 48/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 0.0156 - accuracy: 0.9952 - val_loss: 0.9993 - val_accuracy: 0.9015\n",
            "Epoch 49/100\n",
            "375/375 [==============================] - 2s 7ms/step - loss: 0.0016 - accuracy: 0.9995 - val_loss: 0.9962 - val_accuracy: 0.9093\n",
            "Epoch 50/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 4.8029e-04 - accuracy: 0.9999 - val_loss: 1.0191 - val_accuracy: 0.9067\n",
            "Epoch 51/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 1.6243e-04 - accuracy: 1.0000 - val_loss: 1.0211 - val_accuracy: 0.9100\n",
            "Epoch 52/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 1.0941e-04 - accuracy: 1.0000 - val_loss: 1.0274 - val_accuracy: 0.9083\n",
            "Epoch 53/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 9.0801e-05 - accuracy: 1.0000 - val_loss: 1.0334 - val_accuracy: 0.9088\n",
            "Epoch 54/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 7.3919e-05 - accuracy: 1.0000 - val_loss: 1.0413 - val_accuracy: 0.9093\n",
            "Epoch 55/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 6.2300e-05 - accuracy: 1.0000 - val_loss: 1.0463 - val_accuracy: 0.9097\n",
            "Epoch 56/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 5.2727e-05 - accuracy: 1.0000 - val_loss: 1.0511 - val_accuracy: 0.9107\n",
            "Epoch 57/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 4.5308e-05 - accuracy: 1.0000 - val_loss: 1.0582 - val_accuracy: 0.9103\n",
            "Epoch 58/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 3.8886e-05 - accuracy: 1.0000 - val_loss: 1.0615 - val_accuracy: 0.9112\n",
            "Epoch 59/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 3.3499e-05 - accuracy: 1.0000 - val_loss: 1.0705 - val_accuracy: 0.9107\n",
            "Epoch 60/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 2.8986e-05 - accuracy: 1.0000 - val_loss: 1.0766 - val_accuracy: 0.9113\n",
            "Epoch 61/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 2.4490e-05 - accuracy: 1.0000 - val_loss: 1.0834 - val_accuracy: 0.9097\n",
            "Epoch 62/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 2.1002e-05 - accuracy: 1.0000 - val_loss: 1.0881 - val_accuracy: 0.9108\n",
            "Epoch 63/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 1.7815e-05 - accuracy: 1.0000 - val_loss: 1.0981 - val_accuracy: 0.9110\n",
            "Epoch 64/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 1.5430e-05 - accuracy: 1.0000 - val_loss: 1.1040 - val_accuracy: 0.9118\n",
            "Epoch 65/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 1.2826e-05 - accuracy: 1.0000 - val_loss: 1.1112 - val_accuracy: 0.9105\n",
            "Epoch 66/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 1.1157e-05 - accuracy: 1.0000 - val_loss: 1.1152 - val_accuracy: 0.9108\n",
            "Epoch 67/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 9.3093e-06 - accuracy: 1.0000 - val_loss: 1.1258 - val_accuracy: 0.9100\n",
            "Epoch 68/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 7.7170e-06 - accuracy: 1.0000 - val_loss: 1.1336 - val_accuracy: 0.9112\n",
            "Epoch 69/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 6.6124e-06 - accuracy: 1.0000 - val_loss: 1.1416 - val_accuracy: 0.9100\n",
            "Epoch 70/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 5.5987e-06 - accuracy: 1.0000 - val_loss: 1.1474 - val_accuracy: 0.9098\n",
            "Epoch 71/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 4.6483e-06 - accuracy: 1.0000 - val_loss: 1.1562 - val_accuracy: 0.9102\n",
            "Epoch 72/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 3.9224e-06 - accuracy: 1.0000 - val_loss: 1.1656 - val_accuracy: 0.9098\n",
            "Epoch 73/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 3.3143e-06 - accuracy: 1.0000 - val_loss: 1.1730 - val_accuracy: 0.9098\n",
            "Epoch 74/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 2.7600e-06 - accuracy: 1.0000 - val_loss: 1.1818 - val_accuracy: 0.9100\n",
            "Epoch 75/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 2.3015e-06 - accuracy: 1.0000 - val_loss: 1.1884 - val_accuracy: 0.9097\n",
            "Epoch 76/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 1.9844e-06 - accuracy: 1.0000 - val_loss: 1.1990 - val_accuracy: 0.9100\n",
            "Epoch 77/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 1.6205e-06 - accuracy: 1.0000 - val_loss: 1.2092 - val_accuracy: 0.9103\n",
            "Epoch 78/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 1.3587e-06 - accuracy: 1.0000 - val_loss: 1.2165 - val_accuracy: 0.9108\n",
            "Epoch 79/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 1.1293e-06 - accuracy: 1.0000 - val_loss: 1.2283 - val_accuracy: 0.9112\n",
            "Epoch 80/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 9.4798e-07 - accuracy: 1.0000 - val_loss: 1.2387 - val_accuracy: 0.9098\n",
            "Epoch 81/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 7.9185e-07 - accuracy: 1.0000 - val_loss: 1.2455 - val_accuracy: 0.9098\n",
            "Epoch 82/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 6.5965e-07 - accuracy: 1.0000 - val_loss: 1.2601 - val_accuracy: 0.9107\n",
            "Epoch 83/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 5.5442e-07 - accuracy: 1.0000 - val_loss: 1.2663 - val_accuracy: 0.9102\n",
            "Epoch 84/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 4.4959e-07 - accuracy: 1.0000 - val_loss: 1.2808 - val_accuracy: 0.9103\n",
            "Epoch 85/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 3.8970e-07 - accuracy: 1.0000 - val_loss: 1.2884 - val_accuracy: 0.9113\n",
            "Epoch 86/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 3.0728e-07 - accuracy: 1.0000 - val_loss: 1.2966 - val_accuracy: 0.9108\n",
            "Epoch 87/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 2.5715e-07 - accuracy: 1.0000 - val_loss: 1.3041 - val_accuracy: 0.9105\n",
            "Epoch 88/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 2.1406e-07 - accuracy: 1.0000 - val_loss: 1.3152 - val_accuracy: 0.9108\n",
            "Epoch 89/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 1.8162e-07 - accuracy: 1.0000 - val_loss: 1.3268 - val_accuracy: 0.9107\n",
            "Epoch 90/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 1.4980e-07 - accuracy: 1.0000 - val_loss: 1.3356 - val_accuracy: 0.9108\n",
            "Epoch 91/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 1.2442e-07 - accuracy: 1.0000 - val_loss: 1.3480 - val_accuracy: 0.9105\n",
            "Epoch 92/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 1.0456e-07 - accuracy: 1.0000 - val_loss: 1.3584 - val_accuracy: 0.9103\n",
            "Epoch 93/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 8.7410e-08 - accuracy: 1.0000 - val_loss: 1.3638 - val_accuracy: 0.9107\n",
            "Epoch 94/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 7.3979e-08 - accuracy: 1.0000 - val_loss: 1.3771 - val_accuracy: 0.9113\n",
            "Epoch 95/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 5.9783e-08 - accuracy: 1.0000 - val_loss: 1.3842 - val_accuracy: 0.9112\n",
            "Epoch 96/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 5.1330e-08 - accuracy: 1.0000 - val_loss: 1.3941 - val_accuracy: 0.9112\n",
            "Epoch 97/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 4.2587e-08 - accuracy: 1.0000 - val_loss: 1.4058 - val_accuracy: 0.9118\n",
            "Epoch 98/100\n",
            "375/375 [==============================] - 3s 7ms/step - loss: 3.5450e-08 - accuracy: 1.0000 - val_loss: 1.4137 - val_accuracy: 0.9110\n",
            "Epoch 99/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 3.0289e-08 - accuracy: 1.0000 - val_loss: 1.4266 - val_accuracy: 0.9115\n",
            "Epoch 100/100\n",
            "375/375 [==============================] - 2s 6ms/step - loss: 2.5491e-08 - accuracy: 1.0000 - val_loss: 1.4383 - val_accuracy: 0.9112\n"
          ]
        }
      ],
      "source": [
        "minimum_labeled_samples = [100, 1000, 10000, 30000]\n",
        "\n",
        "\n",
        "for n in minimum_labeled_samples:\n",
        "  model = train_classifier(n)\n",
        "  model_acc = calculate_metrics(model)\n",
        "  with open(\"./results.txt\", mode=\"a\") as f:\n",
        "    f.write(f\"\\n{n}\\t{model_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Plot acc vs sample size of labeled data\n",
        "with open('./results.txt', 'r') as f:\n",
        "  next(f) #header\n",
        "  data = [line.strip('\\n').split('\\t') for line in f]\n",
        "  data = np.array([[float(a), float(b)] for a,b in data])\n",
        "print(data)\n",
        "plt.scatter(x=data[:,0], y=data[:,1], label='accuracy')\n",
        "plt.axhline(y=0.9, color='r', linestyle='--')\n",
        "plt.title('Accuracy')\n",
        "plt.legend()\n",
        "plt.show"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "tLr3afizKZmi",
        "outputId": "6f4e629e-3ddf-4572-bf4b-1ee1fb08ea2d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.000e+02 6.445e-01]\n",
            " [1.000e+03 8.171e-01]\n",
            " [1.000e+04 8.860e-01]\n",
            " [3.000e+04 9.022e-01]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(*args, **kw)>"
            ]
          },
          "metadata": {},
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing the sample size vs accuracy, we probably have the best sample size between 15k-25k to get an accuracy of 0.9+"
      ],
      "metadata": {
        "id": "xJZzEZQjQzpr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "CAgSU4QNm7R8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9aeb6e7f-684a-47b3-bad3-c4fa8aaa30fb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.84      0.84      0.84      1000\n",
            "     Trouser       0.99      0.98      0.99      1000\n",
            "    Pullover       0.85      0.85      0.85      1000\n",
            "       Dress       0.91      0.89      0.90      1000\n",
            "        Coat       0.83      0.85      0.84      1000\n",
            "      Sandal       0.98      0.98      0.98      1000\n",
            "       Shirt       0.74      0.73      0.74      1000\n",
            "     Sneaker       0.95      0.97      0.96      1000\n",
            "         Bag       0.97      0.97      0.97      1000\n",
            "  Ankle boot       0.97      0.95      0.96      1000\n",
            "\n",
            "    accuracy                           0.90     10000\n",
            "   macro avg       0.90      0.90      0.90     10000\n",
            "weighted avg       0.90      0.90      0.90     10000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "## Classification Report to check what classes are performing/under-performing\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "full_model = load_model('./models/classification_complete_30000.h5')\n",
        "\n",
        "predicted_classes = full_model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "target_names = [f\"{label_dict[i]}\".format(i) for i in range(num_classes)]\n",
        "print(classification_report(y_test, predicted_classes, target_names=target_names))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Checking the results for 10k samples model\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "full_model = load_model('./models/classification_complete_10000.h5')\n",
        "\n",
        "predicted_classes = full_model.predict(x_test)\n",
        "predicted_classes = np.argmax(np.round(predicted_classes),axis=1)\n",
        "\n",
        "target_names = [f\"{label_dict[i]}\".format(i) for i in range(num_classes)]\n",
        "print(classification_report(y_test, predicted_classes, target_names=target_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdqF3M1vSoJF",
        "outputId": "9c7f2227-d841-4a57-fb97-3600863bbfe5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            " T-shirt/top       0.84      0.81      0.82      1000\n",
            "     Trouser       0.99      0.96      0.98      1000\n",
            "    Pullover       0.82      0.84      0.83      1000\n",
            "       Dress       0.87      0.89      0.88      1000\n",
            "        Coat       0.80      0.84      0.82      1000\n",
            "      Sandal       0.96      0.96      0.96      1000\n",
            "       Shirt       0.73      0.68      0.70      1000\n",
            "     Sneaker       0.94      0.93      0.94      1000\n",
            "         Bag       0.97      0.97      0.97      1000\n",
            "  Ankle boot       0.95      0.96      0.96      1000\n",
            "\n",
            "    accuracy                           0.89     10000\n",
            "   macro avg       0.89      0.89      0.89     10000\n",
            "weighted avg       0.89      0.89      0.89     10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Closing Notes:\n",
        "1. This is just one approach to reduce the number of labeled samples needed for classification. Looking at the report for 30k samples, we see that some classes (like Shirt) are underperforming, whereas a few (like Trouser) are very good. Instead of randomly sampling from training set, if we were to carefully sample from each class, we can cut down the \"n\"\n",
        "2. The 10k model is almost there, more tweaks to improve underperforming classes and we get 0.9 accuracy on the test set.\n",
        "3. Parameter and Hyperparameter tuning should improve results further.\n",
        "4. Trainable/Untrainable: with the initial layers set to trainable, the accuracy metrics are slighty better, the results are in ./other\n",
        "5. I have limited experience with autoencs, this was a fun problem to work on."
      ],
      "metadata": {
        "id": "w8Le1C7ZV65e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip freeze > requirements.txt"
      ],
      "metadata": {
        "id": "eYyBF5_hTLoX"
      },
      "execution_count": 34,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "fashion_mnist.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}